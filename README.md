# Smiling Face Detector with Eigenfaces
In this study, an introduction to the eigenfaces approach, which is one of the first techniques for face recognition and based on PCA, will be introduced and a case study will be made. <br/>

## Eigenfaces
When applied to the computer vision problem of recognizing human faces, a set of eigenvectors are referred to as an eigenface. Sirovich and Kirby created the method of employing eigenfaces for recognition, then Matthew Turk and Alex Pentland used it to face classification. The probability distribution across the high-dimensional vector space of face photos is used to derive the eigenvectors from the covariance matrix. All of the images used to create the covariance matrix are based on the eigenfaces. By enabling the reduced number of basis images to represent the initial training images, this results in dimension reduction. Comparing how the basis set represents faces allows for classification.
### Generation
Principal component analysis (PCA), a mathematical technique, can be used on a huge collection of photographs showing various human faces to produce a set of eigenfaces. Informally, eigenfaces are a collection of "standardized face constituents" that were discovered by statistical analysis of several facial portraits. Any human face can be thought of as a combination of several common facial features. The average face plus 10% from Eigenface 1, 55% from Eigenface 2, and even 3% from Eigenface 3 could make up one's face, as an illustration. Surprisingly, a decent approximation of the majority of faces can be obtained by combining a small number of eigenfaces.  Also, because a person's face is not recorded by a digital photograph, but instead as just a list of values (one value for each eigenface in the database used), much less space is taken for each person's face. <br/>
The resulting eigenfaces will manifest as a certain arrangement of light and dark regions. This pattern is used to identify certain facial features for analysis and scoring. There will be a pattern to assess symmetry, the presence or absence of facial hair, the location of the hairline, and the size of the mouth and nose. Some eigenfaces contain patterns that are harder to spot, and their images might not even remotely resemble human faces.
### Algorithm
1- Create a training collection of pictures. The photographs that make up the training set must have been captured in identical lighting circumstances and standardized so that the mouths and eyes are consistent across all images. Also, they must all be resampled to the same pixel resolution (r x c). By simply concatenating the rows of pixels in the original image, which creates a single column with r x c elements, each image is considered as one vector. For the purposes of this implementation, it is assumed that the training set's photos are all kept in a single matrix T, with an image occupying each column. <br/>
2- Subtract the mean. The average image a has to be calculated and then subtracted from each original image in T. <br/>
3- Determine the covariance matrix S's eigenvalues and eigenvectors. Every eigenvector is an image in and of itself because it has the same dimensionality as the original images. Hence, eigenfaces refers to the eigenvectors of this covariance matrix. These represent the axes along which the images deviate from the average image. <br/>
4- Choose the principal components. Sort the eigenvalues in descending order and arrange eigenvectors accordingly. The number of principal components k is determined arbitrarily by setting a threshold ε on the total variance. Total variance v = (lambda1, lambda2 , ..., lambda_n) where n = number of components <br/>
5- k is the smallest number that satisfies (lambda1, lambda2 , ..., lambda_k)/v > ε  <br/>
The dataset or alternative datasets to be used for this study can be downloaded from the link below or used by downloading the 'Yale_32x32.mat' file included in the files. <br/>
Now, we may project a new (mean-subtracted) picture on the eigenfaces and record how that new face differs from the mean face, allowing us to use these eigenfaces to represent both old and new faces. The distances between each eigenface's associated eigenvalue and mean picture in the training set are measured in that direction. By preserving the eigenfaces with the highest eigenvalues, losses are kept to a minimum when the picture is projected on a subset of the eigenvectors. For instance, 10,000 eigenvectors will be generated while working with a 100 100 image. Most of the 10,000 eigenvectors can be deleted in practical applications because most faces can often be recognized using a projection on between 100 and 150 eigenfaces. <br/>
## Case Study
In this study, 'Smiling' or 'Neutral' faces, which can be obtained from the link below or from the 'Yale32x32.mat' file, will be classified. <br/>
LINK : http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html <br/>
The problem will be treated as a two-class classification problem, 'Neutral' and 'Smiling' faces. Therefore, first of all, the data set was displayed with the help of 'display_faces.m' codes. The 'Neutral' and 'Smiling' markings on the displayed data set can be seen in the 'Labelling the faces' section of the Codes.m file. After labelling, the 'Mean Face' is substract first. The 'Mean Face' of the faces in the dataset can be seen below: <br/>
![image](https://user-images.githubusercontent.com/78887209/221875855-89734b3f-90eb-43e4-9121-6cbbc6c2493d.png) <br/>
Then, eigen vectors and values were found with the help of Singular Value Decomposition. The first 3 eigenfaces obtained with the help of eigen vectors and values can be seen below: <br/>
![image](https://user-images.githubusercontent.com/78887209/221897329-06d20138-b0ad-480b-8301-99c6e4259d98.png) <br/>
As explained in the introduction, the number of principal components required to model 90% of the data variance was determined. And these principal components are used in classification. This saves memory and time. Eigenvectors, which can represent 90% of the data variance, are projected onto the corresponding eigenfaces of each of the neutral and smiling faces. With the help of the coefficients of the projected projection matrix, each face is classified by supervised learning and each face in the data set is correctly classified.
### Further Readings
https://en.wikipedia.org/wiki/Eigenface <br/>
https://www.quora.com/How-are-Eigenvectors-and-Eigenvalues-used-in-image-processing
